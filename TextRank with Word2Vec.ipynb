{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93aec5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizing import get_tokenized_sentences, get_raw_sentences\n",
    "\n",
    "raw_sentences = get_raw_sentences(\"preprocessed/textrank.txt\") \n",
    "sentence_tokens = get_tokenized_sentences(raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da1005e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00067248  0.00104613  0.00487297  0.00757618 -0.00803934 -0.01106264\n",
      "  0.00756057  0.01615757 -0.00792757 -0.00561812  0.00699328 -0.00689477\n",
      " -0.00340897  0.00982612 -0.00476033 -0.00314174  0.00514402 -0.00323009\n",
      " -0.00905114 -0.01615958  0.00964394  0.00726432  0.0070368  -0.00143415\n",
      "  0.00612977 -0.00403449 -0.0025043   0.00457922 -0.00865229 -0.00530687\n",
      " -0.00476261 -0.00031019  0.01091721 -0.0114891  -0.00415985  0.00310229\n",
      "  0.01054732 -0.00919225 -0.00040217 -0.01025883 -0.00871832  0.00267587\n",
      " -0.01017513 -0.00441323  0.00324162 -0.00109886 -0.01167373  0.00991396\n",
      "  0.00707147  0.01008213 -0.00946825  0.00217995 -0.0036368   0.00170181\n",
      "  0.00834085 -0.00243042  0.00576255 -0.00524199 -0.00591708  0.01159949\n",
      "  0.00030088  0.00068237 -0.00172966 -0.00697377 -0.00477636  0.00601895\n",
      " -0.00066239  0.00896309 -0.00678463  0.0056695   0.00415258  0.01065679\n",
      "  0.00064948 -0.00990027  0.00830606  0.00106054  0.00855871  0.0006148\n",
      " -0.00705622 -0.00796586 -0.00204225  0.00490391  0.00202641  0.01233112\n",
      " -0.00887261  0.00232727  0.01056093 -0.00164324  0.00154519  0.00827178\n",
      "  0.00544866  0.00331807  0.00406268  0.00103143  0.01364105  0.00830427\n",
      " -0.00649938 -0.0081435   0.00226501  0.00668169]\n",
      "[('directed', 0.42462849617004395), ('given', 0.3961712419986725), ('units', 0.3562217950820923), ('process', 0.35148707032203674), ('convergence', 0.3089504837989807), ('graphs', 0.3058125674724579), ('added', 0.30043312907218933), ('v', 0.2964668273925781), ('sentences', 0.27924126386642456), ('data', 0.27278146147727966)]\n",
      "['text', 'textrank', 'graph', 'extraction', 'words', 'set', 'vertex', 'ranking', 'keywords', 'units', 'keyword', 'results', 'sentences', 'sentence', 'two', 'vertices', 'given', 'lexical', 'systems', 'system', 'number', 'graphbased', 'used', 'algorithm', 'score', 'model', 'also', 'using', 'graphs', 'natural', 'document', 'information', 'added', 'language', 'convergence', 'application', 'important', 'relation', 'based', 'syntactic', 'approach', 'web', 'algorithms', 'abstracts', 'summaries', 'task', 'one', 'extracted', 'therefore', 'obtained', 'figure', 'summary', 'supervised', 'data', 'experiments', 'evaluation', 'summarization', 'links', 'achieved', 'drawn', 'entire', 'since', 'edges', 'nouns', 'similar', 'v', 'importance', 'learning', 'associated', 'notice', 'table', 'assigned', 'directed', 'candidate', 'part', 'applied', 'section', 'precision', 'speech', 'cooccurrence', 'methods', 'unsupervised', 'applications', 'computed', 'selected', 'edge', 'curves', 'consisting', 'another', 'word', 'texts', 'recommendation', 'may', 'sample', 'build', 'proposed', 'process', 'best', 'linear', 'connections', 'inequations', 'weights', 'abstract', 'first', 'eg', 'context', 'different', 'iterations', 'show', 'window', 'five', 'previously', 'similarity', 'shows', 'size', 'ranked', 'filters', 'makes', 'processing', 'training', 'single', 'recall', 'top', 'test', 'defined', 'within', 'page', 'relations', 'higher', 'threshold', 'concepts', 'undirected', 'connected', 'overlap', 'useful', 'extractive', 'various', 'found', 'consists', 'following', 'recursively', 'automatic', 'account', 'collection', 'random', 'frequency', 'highly']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentence_tokens)\n",
    "\n",
    "# get numpy vector of word\n",
    "print(model.wv[0])\n",
    "\n",
    "# get top 10 most similar words to 'text'\n",
    "print(model.wv.most_similar('text', topn=10))\n",
    "print(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae77b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence in sentence_tokens:\n",
    "    if len(sentence) != 0:\n",
    "        word_vecs = []\n",
    "        for word in sentence:\n",
    "            if word in model.wv.index_to_key:\n",
    "                word_vecs.append(model.wv[word])\n",
    "            else:\n",
    "                word_vecs.append(np.zeros(100,))\n",
    "        vec = sum(word_vecs)/(len(word_vecs)+0.001)\n",
    "    else:\n",
    "        vec = np.zeros((100,))\n",
    "    sentence_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "934435ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_mat = np.zeros([len(raw_sentences), len(raw_sentences)])\n",
    "for i in range(len(raw_sentences)):\n",
    "    for j in range(len(raw_sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6c4a70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc. Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.\n",
      "\n",
      "Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.\n",
      "\n",
      "Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).\n",
      "\n",
      "In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept C' in a text, we are likely to “follow” links to connected concepts — that is, concepts that have a relation with the current concept C (be that a lexical or semantic relation).\n",
      "\n",
      "The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.\n",
      "\n",
      "TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.\n",
      "\n",
      "Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between co 3The fact that the supervised system does not have the capability to set a cutoff threshold on the number of keywords, but it only makes a binary decision on each candidate word, has the downside of not allowing for a precision-recall curve, which prohibits a comparison of such curves for the two methods.\n",
      "\n",
      "During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.\n",
      "\n",
      "Another important aspect of TextRank is that it gives a ranking over all sentences in a text — which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.\n",
      "\n",
      "Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(raw_sentences)), reverse=True)\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee3370c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc. Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.\n",
      "\n",
      "The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.\n",
      "\n",
      "During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.\n",
      "\n",
      "Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.\n",
      "\n",
      "Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between co 3The fact that the supervised system does not have the capability to set a cutoff threshold on the number of keywords, but it only makes a binary decision on each candidate word, has the downside of not allowing for a precision-recall curve, which prohibits a comparison of such curves for the two methods.\n",
      "\n",
      "TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.\n",
      "\n",
      "Another important aspect of TextRank is that it gives a ranking over all sentences in a text — which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.\n",
      "\n",
      "Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).\n",
      "\n",
      "In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept C' in a text, we are likely to “follow” links to connected concepts — that is, concepts that have a relation with the current concept C (be that a lexical or semantic relation).\n",
      "\n",
      "Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_sentences = sorted(((scores[i],s) for i,s in enumerate(raw_sentences)), reverse=True)\n",
    "\n",
    "num_to_print = 10\n",
    "ranked_sentences = [sentence for score, sentence in scored_sentences]\n",
    "\n",
    "# num_printed = 0\n",
    "for i in range(len(raw_sentences)):\n",
    "    if raw_sentences[i] in ranked_sentences[:10]:\n",
    "        print(raw_sentences[i])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Capstone]",
   "language": "python",
   "name": "conda-env-Capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
