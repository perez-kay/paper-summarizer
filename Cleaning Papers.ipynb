{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This did NOT work, the text had way too many new lines in it. Removing them all gave me one giant sentence\n",
    "# with no spaces\n",
    "\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "\n",
    "reader = PdfFileReader(\"textrank algorithm paper.pdf\")\n",
    "num_pages = reader.numPages\n",
    "page = reader.pages[0]\n",
    "re.sub(\"\\n\", \"\", page.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f06bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This had \n",
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"textrank algorithm paper.pdf\") as pdf:\n",
    "    page = pdf.pages[0]\n",
    "    left = page.crop((0, 0, 0.5 * page.width, 0.9 * page.height))\n",
    "    right = page.crop((0.5 * page.width, 0, page.width, page.height))\n",
    "    l_text = left.extract_text()\n",
    "    r_text = right.extract_text()\n",
    "    text = l_text + \" \" + r_text\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29128ef9",
   "metadata": {},
   "source": [
    "Code below was obtained from [here](https://pdfminersix.readthedocs.io/en/latest/tutorial/composable.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d921af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank: Bringing Order into Texts\n",
      "\n",
      "Rada Mihalcea and Paul Tarau\n",
      "Department of Computer Science\n",
      "University of North Texas\n",
      "rada,tarau (cid:1) @cs.unt.edu\n",
      "\n",
      "Abstract\n",
      "\n",
      "In this paper, we introduce TextRank – a graph-based\n",
      "ranking model for text processing, and show how this\n",
      "model can be successfully used in natural language\n",
      "applications. In particular, we propose two innova-\n",
      "tive unsupervised methods for keyword and sentence\n",
      "extraction, and show that the results obtained com-\n",
      "pare favorably with previously published results on\n",
      "established benchmarks.\n",
      "\n",
      "1 Introduction\n",
      "\n",
      "Graph-based ranking algorithms like Kleinberg’s\n",
      "HITS algorithm (Kleinberg, 1999) or Google’s\n",
      "PageRank (Brin and Page, 1998) have been success-\n",
      "fully used in citation analysis, social networks, and\n",
      "the analysis of the link-structure of the World Wide\n",
      "Web. Arguably, these algorithms can be singled out\n",
      "as key elements of the paradigm-shift triggered in\n",
      "the ﬁeld of Web search technology, by providing a\n",
      "Web page ranking mechanism that relies on the col-\n",
      "lective knowledge of Web architects rather than in-\n",
      "dividual content analysis of Web pages. In short, a\n",
      "graph-based ranking algorithm is a way of deciding\n",
      "on the importance of a vertex within a graph, by tak-\n",
      "ing into account global information recursively com-\n",
      "puted from the entire graph, rather than relying only\n",
      "on local vertex-speciﬁc information.\n",
      "\n",
      "Applying a similar line of thinking to lexical\n",
      "or semantic graphs extracted from natural language\n",
      "documents, results in a graph-based ranking model\n",
      "that can be applied to a variety of natural language\n",
      "processing applications, where knowledge drawn\n",
      "from an entire text is used in making local rank-\n",
      "ing/selection decisions. Such text-oriented ranking\n",
      "methods can be applied to tasks ranging from auto-\n",
      "mated extraction of keyphrases, to extractive summa-\n",
      "rization and word sense disambiguation (Mihalcea et\n",
      "al., 2004).\n",
      "\n",
      "In this paper, we introduce the TextRank graph-\n",
      "based ranking model for graphs extracted from nat-\n",
      "ural language texts. We investigate and evaluate the\n",
      "application of TextRank to two language processing\n",
      "tasks consisting of unsupervised keyword and sen-\n",
      "\n",
      "tence extraction, and show that the results obtained\n",
      "with TextRank are competitive with state-of-the-art\n",
      "systems developed in these areas.\n",
      "\n",
      "2 The TextRank Model\n",
      "\n",
      "Graph-based ranking algorithms are essentially a\n",
      "way of deciding the importance of a vertex within\n",
      "a graph, based on global information recursively\n",
      "drawn from the entire graph. The basic idea im-\n",
      "plemented by a graph-based ranking model is that\n",
      "of “voting” or “recommendation”. When one ver-\n",
      "tex links to another one, it is basically casting a vote\n",
      "for that other vertex. The higher the number of votes\n",
      "that are cast for a vertex, the higher the importance\n",
      "of the vertex. Moreover, the importance of the vertex\n",
      "casting the vote determines how important the vote\n",
      "itself is, and this information is also taken into ac-\n",
      "count by the ranking model. Hence, the score asso-\n",
      "ciated with a vertex is determined based on the votes\n",
      "that are cast for it, and the score of the vertices cast-\n",
      "ing these votes.\n",
      "\n",
      "Formally, let (cid:2)(cid:4)(cid:3)(cid:6)(cid:5)(cid:8)(cid:7)(cid:10)(cid:9)(cid:12)(cid:11)(cid:14)(cid:13) be a directed graph with\n",
      "is a\n",
      "and set of edges (cid:11)\n",
      "the set of vertices (cid:7)\n",
      "subset of (cid:7)(cid:16)(cid:15)(cid:17)(cid:7)\n",
      ". For a given vertex (cid:7)(cid:19)(cid:18) , let (cid:20)(cid:22)(cid:21)(cid:23)(cid:5)(cid:8)(cid:7)(cid:24)(cid:18)(cid:8)(cid:13) be\n",
      "the set of vertices that point to it (predecessors), and\n",
      "(cid:18) points\n",
      "(cid:13) be the set of vertices that vertex (cid:7)\n",
      "let (cid:25)(cid:27)(cid:26)(cid:29)(cid:28)(cid:30)(cid:5)(cid:8)(cid:7)\n",
      "to (successors). The score of a vertex (cid:7)\n",
      "(cid:18) is deﬁned as\n",
      "follows (Brin and Page, 1998):\n",
      "\n",
      ", where (cid:11)\n",
      "\n",
      "(cid:31)! #\"%$#&(cid:24)’( *),+(cid:14)-.&(cid:22)/0-21\n",
      "\n",
      "(cid:31)K #\"\n",
      "\n",
      "BDCFE\n",
      "\n",
      ":G;IHJ>\n",
      "\n",
      "46587(cid:8)9.:<;(cid:12)=?>\n",
      "\n",
      "is a damping factor that can be set between\n",
      "where L\n",
      "0 and 1, which has the role of integrating into the\n",
      "model the probability of jumping from a given vertex\n",
      "to another random vertex in the graph. In the context\n",
      "of Web surﬁng, this graph-based ranking algorithm\n",
      "implements the “random surfer model”, where a user\n",
      "clicks on links at random with a probability L , and\n",
      "jumps to a completely new page with probability MON\n",
      "is usually set to 0.85 (Brin and Page,\n",
      "L . The factor L\n",
      "1998), and this is the value we are also using in our\n",
      "implementation.\n",
      "\n",
      "(cid:0)\n",
      "(cid:18)\n",
      "3\n",
      "@\n",
      "A\n",
      "A\n",
      "4\n",
      "&\n",
      "\f",
      "Starting from arbitrary values assigned to each\n",
      "node in the graph, the computation iterates until con-\n",
      "vergence below a given threshold is achieved 1. After\n",
      "running the algorithm, a score is associated with each\n",
      "vertex, which represents the “importance” of the ver-\n",
      "tex within the graph. Notice that the ﬁnal values\n",
      "obtained after TextRank runs to completion are not\n",
      "affected by the choice of the initial value, only the\n",
      "number of iterations to convergence may be differ-\n",
      "ent.\n",
      "\n",
      "It is important to notice that although the TextRank\n",
      "applications described in this paper rely on an al-\n",
      "gorithm derived from Google’s PageRank (Brin and\n",
      "Page, 1998), other graph-based ranking algorithms\n",
      "such as e.g. HITS (Kleinberg, 1999) or Positional\n",
      "Function (Herings et al., 2001) can be easily inte-\n",
      "grated into the TextRank model (Mihalcea, 2004).\n",
      "\n",
      "2.1 Undirected Graphs\n",
      "\n",
      "Although traditionally applied on directed graphs, a\n",
      "recursive graph-based ranking algorithm can be also\n",
      "applied to undirected graphs, in which case the out-\n",
      "degree of a vertex is equal to the in-degree of the ver-\n",
      "tex. For loosely connected graphs, with the number\n",
      "of edges proportional with the number of vertices,\n",
      "undirected graphs tend to have more gradual conver-\n",
      "gence curves.\n",
      "\n",
      "Figure 1 plots the convergence curves for a ran-\n",
      "domly generated graph with 250 vertices and 250\n",
      "edges, for a convergence threshold of 0.0001. As the\n",
      "connectivity of the graph increases (i.e. larger num-\n",
      "ber of edges), convergence is usually achieved after\n",
      "fewer iterations, and the convergence curves for di-\n",
      "rected and undirected graphs practically overlap.\n",
      "\n",
      "2.2 Weighted Graphs\n",
      "\n",
      "In the context of Web surﬁng, it is unusual for a\n",
      "page to include multiple or partial links to another\n",
      "page, and hence the original PageRank deﬁnition for\n",
      "graph-based ranking is assuming unweighted graphs.\n",
      "However, in our model the graphs are build from\n",
      "natural language texts, and may include multiple or\n",
      "partial links between the units (vertices) that are ex-\n",
      "tracted from text. It may be therefore useful to in-\n",
      "dicate and incorporate into the model the “strength”\n",
      "(cid:1)(cid:0) as\n",
      "of the connection between two vertices (cid:7)\n",
      "a weight (cid:2)\n",
      "(cid:0) added to the corresponding edge that\n",
      "connects the two vertices.\n",
      "\n",
      "(cid:18) and (cid:7)\n",
      "\n",
      "(cid:31)K #\"%$?&\n",
      "\n",
      "1Convergence is achieved when the error rate for any vertex\n",
      "in the graph falls below a given threshold. The error rate of a\n",
      "is deﬁned as the difference between the “real” score of\n",
      "vertex\n",
      "the vertex\n",
      ".\n",
      "Since the real score is not known apriori, this error rate is ap-\n",
      "proximated with the difference between the scores computed at\n",
      "two successive iterations:\n",
      "\n",
      "and the score computed at iteration (cid:3)\n",
      "\n",
      ",\n",
      "\n",
      ".\n",
      "\n",
      ". #\"%$#&\n",
      "\n",
      " #\"%$#&\n",
      "\n",
      " #\"%$#&\n",
      "\n",
      "(cid:5)(cid:4)(cid:7)(cid:6)\n",
      "\n",
      "+0(cid:31)\n",
      "\n",
      "(cid:5)(cid:4)\n",
      "\n",
      "(cid:8)(cid:4)\n",
      "\n",
      "Convergence curves (250 vertices, 250 edges)\n",
      "\n",
      "e\n",
      "t\n",
      "a\n",
      "r\n",
      "\n",
      "r\n",
      "o\n",
      "r\n",
      "r\n",
      "\n",
      "E\n",
      "\n",
      " 0.18\n",
      "\n",
      " 0.16\n",
      "\n",
      " 0.14\n",
      "\n",
      " 0.12\n",
      "\n",
      " 0.1\n",
      "\n",
      " 0.08\n",
      "\n",
      " 0.06\n",
      "\n",
      " 0.04\n",
      "\n",
      " 0.02\n",
      "\n",
      " 0\n",
      "\n",
      " 0\n",
      "\n",
      "undirected/unweighted\n",
      "undirected/weighted\n",
      "directed/unweighted\n",
      "directed/weighted\n",
      "\n",
      " 5\n",
      "\n",
      " 10\n",
      "\n",
      " 15\n",
      "\n",
      " 20\n",
      "\n",
      " 25\n",
      "\n",
      " 30\n",
      "\n",
      "Number of iterations\n",
      "\n",
      "Figure 1: Convergence curves for graph-based\n",
      "ranking: directed/undirected, weighted/unweighted\n",
      "graph, 250 vertices, 250 edges.\n",
      "\n",
      "Consequently, we introduce a new formula for\n",
      "graph-based ranking that takes into account edge\n",
      "weights when computing the score associated with\n",
      "a vertex in the graph. Notice that a similar formula\n",
      "can be deﬁned to integrate vertex weights.\n",
      "\n",
      "(cid:31)K #\"%$\n",
      "\n",
      "&(cid:29)’( *)!+(cid:14)-.&(cid:22)/0-21\n",
      "\n",
      "(cid:31)! #\"\n",
      "\n",
      "587(cid:8)9.:<;(cid:12)=?>\n",
      "\n",
      "(cid:11)(cid:13)(cid:12)(cid:15)(cid:14)(cid:17)(cid:16)(cid:19)(cid:18)(cid:21)(cid:20)(cid:23)(cid:22)(cid:24)(cid:11)\n",
      "\n",
      "(cid:7)(cid:25)\n",
      "\n",
      "Figure 1 plots the convergence curves for the same\n",
      "sample graph from section 2.1, with random weights\n",
      "in the interval 0–10 added to the edges. While the ﬁ-\n",
      "nal vertex scores (and therefore rankings) differ sig-\n",
      "niﬁcantly as compared to their unweighted alterna-\n",
      "tives, the number of iterations to convergence and the\n",
      "shape of the convergence curves is almost identical\n",
      "for weighted and unweighted graphs.\n",
      "\n",
      "2.3 Text as a Graph\n",
      "\n",
      "To enable the application of graph-based ranking\n",
      "algorithms to natural language texts, we have to\n",
      "build a graph that represents the text, and intercon-\n",
      "nects words or other text entities with meaningful\n",
      "relations. Depending on the application at hand,\n",
      "text units of various sizes and characteristics can be\n",
      "added as vertices in the graph, e.g. words, colloca-\n",
      "tions, entire sentences, or others. Similarly, it is the\n",
      "application that dictates the type of relations that are\n",
      "used to draw connections between any two such ver-\n",
      "tices, e.g.\n",
      "lexical or semantic relations, contextual\n",
      "overlap, etc.\n",
      "\n",
      "Regardless of the type and characteristics of the el-\n",
      "ements added to the graph, the application of graph-\n",
      "based ranking algorithms to natural language texts\n",
      "consists of the following main steps:\n",
      "\n",
      "(cid:18)\n",
      "\"\n",
      "$\n",
      "(cid:31)\n",
      "(cid:31)\n",
      "@\n",
      " \n",
      "(cid:9)\n",
      "3\n",
      ";\n",
      "H\n",
      "(cid:10)\n",
      "H\n",
      "=\n",
      "3\n",
      "H\n",
      "(cid:10)\n",
      "H\n",
      "(cid:12)\n",
      "(cid:9)\n",
      "4\n",
      "&\n",
      "\f",
      "1. Identify text units that best deﬁne the task at hand,\n",
      "\n",
      "andaddthemasverticesinthegraph.\n",
      "\n",
      "2. Identify relations that connect such text units, and\n",
      "use these relations to draw edges between vertices\n",
      "in the graph. Edges can be directed or undirected,\n",
      "weightedorunweighted.\n",
      "\n",
      "3. Iteratethe graph-basedrankingalgorithmuntilcon-\n",
      "\n",
      "vergence.\n",
      "\n",
      "4. Sortverticesbasedontheir ﬁnal score. Usethe val-\n",
      "uesattachedtoeachvertexforranking/selectionde-\n",
      "cisions.\n",
      "\n",
      "In the following, we investigate and evaluate the\n",
      "application of TextRank to two natural language pro-\n",
      "cessing tasks involving ranking of text units: (1) A\n",
      "keyword extraction task, consisting of the selection\n",
      "of keyphrases representative for a given text; and (2)\n",
      "A sentence extraction task, consisting of the identi-\n",
      "ﬁcation of the most “important” sentences in a text,\n",
      "which can be used to build extractive summaries.\n",
      "\n",
      "3 Keyword Extraction\n",
      "The task of a keyword extraction application is to au-\n",
      "tomatically identify in a text a set of terms that best\n",
      "describe the document. Such keywords may consti-\n",
      "tute useful entries for building an automatic index for\n",
      "a document collection, can be used to classify a text,\n",
      "or may serve as a concise summary for a given doc-\n",
      "ument. Moreover, a system for automatic identiﬁca-\n",
      "tion of important terms in a text can be used for the\n",
      "problem of terminology extraction, and construction\n",
      "of domain-speciﬁc dictionaries.\n",
      "\n",
      "The simplest possible approach is perhaps to use\n",
      "a frequency criterion to select the “important” key-\n",
      "words in a document. However, this method was\n",
      "generally found to lead to poor results, and conse-\n",
      "quently other methods were explored. The state-of-\n",
      "the-art in this area is currently represented by super-\n",
      "vised learning methods, where a system is trained to\n",
      "recognize keywords in a text, based on lexical and\n",
      "syntactic features. This approach was ﬁrst suggested\n",
      "in (Turney, 1999), where parametrized heuristic rules\n",
      "are combined with a genetic algorithm into a sys-\n",
      "tem for keyphrase extraction - GenEx - that automat-\n",
      "ically identiﬁes keywords in a document. A different\n",
      "learning algorithm was used in (Frank et al., 1999),\n",
      "where a Naive Bayes learning scheme is applied on\n",
      "the document collection, with improved results ob-\n",
      "served on the same data set as used in (Turney, 1999).\n",
      "Neither Turney nor Frank report on the recall of\n",
      "their systems, but only on precision: a 29.0% preci-\n",
      "sion is achieved with GenEx (Turney, 1999) for ﬁve\n",
      "keyphrases extracted per document, and 18.3% pre-\n",
      "cision achieved with Kea (Frank et al., 1999) for ﬁf-\n",
      "teen keyphrases per document.\n",
      "\n",
      "More recently, (Hulth, 2003) applies a super-\n",
      "vised learning system to keyword extraction from ab-\n",
      "\n",
      "stracts, using a combination of lexical and syntactic\n",
      "features, proved to improve signiﬁcantly over previ-\n",
      "ously published results. As Hulth suggests, keyword\n",
      "extraction from abstracts is more widely applicable\n",
      "than from full texts, since many documents on the\n",
      "Internet are not available as full-texts, but only as\n",
      "abstracts. In her work, Hulth experiments with the\n",
      "approach proposed in (Turney, 1999), and a new ap-\n",
      "proach that integrates part of speech information into\n",
      "the learning process, and shows that the accuracy of\n",
      "the system is almost doubled by adding linguistic\n",
      "knowledge to the term representation.\n",
      "\n",
      "In this section, we report on our experiments in\n",
      "keyword extraction using TextRank, and show that\n",
      "the graph-based ranking model outperforms the best\n",
      "published results in this problem. Similar to (Hulth,\n",
      "2003), we are evaluating our algorithm on keyword\n",
      "extraction from abstracts, mainly for the purpose of\n",
      "allowing for a direct comparison with the results she\n",
      "reports with her keyphrase extraction system. Notice\n",
      "that the size of the text is not a limitation imposed\n",
      "by our system, and similar results are expected with\n",
      "TextRank applied on full-texts.\n",
      "\n",
      "3.1 TextRank for Keyword Extraction\n",
      "\n",
      "The expected end result for this application is a set of\n",
      "words or phrases that are representative for a given\n",
      "natural language text. The units to be ranked are\n",
      "therefore sequences of one or more lexical units ex-\n",
      "tracted from text, and these represent the vertices that\n",
      "are added to the text graph. Any relation that can\n",
      "be deﬁned between two lexical units is a potentially\n",
      "useful connection (edge) that can be added between\n",
      "two such vertices. We are using a co-occurrence re-\n",
      "lation, controlled by the distance between word oc-\n",
      "currences: two vertices are connected if their corre-\n",
      "sponding lexical units co-occur within a window of\n",
      "maximum (cid:0) words, where (cid:0)\n",
      "can be set anywhere\n",
      "from 2 to 10 words. Co-occurrence links express\n",
      "relations between syntactic elements, and similar to\n",
      "the semantic links found useful for the task of word\n",
      "sense disambiguation (Mihalcea et al., 2004), they\n",
      "represent cohesion indicators for a given text.\n",
      "\n",
      "The vertices added to the graph can be restricted\n",
      "with syntactic ﬁlters, which select only lexical units\n",
      "of a certain part of speech. One can for instance con-\n",
      "sider only nouns and verbs for addition to the graph,\n",
      "and consequently draw potential edges based only on\n",
      "relations that can be established between nouns and\n",
      "verbs. We experimented with various syntactic ﬁl-\n",
      "ters, including: all open class words, nouns and verbs\n",
      "only, etc., with best results observed for nouns and\n",
      "adjectives only, as detailed in section 3.2.\n",
      "\n",
      "The TextRank keyword extraction algorithm is\n",
      "fully unsupervised, and proceeds as follows. First,\n",
      "\n",
      "\f",
      "Compatibility of systems of linear constraints over the set of natural numbers. \n",
      "Criteria of compatibility of a system of linear Diophantine equations, strict\n",
      "inequations, and nonstrict inequations are considered. Upper bounds for\n",
      "components of a minimal set of solutions and algorithms of construction of\n",
      "minimal generating sets of solutions for all types of systems are given. \n",
      "These criteria and the corresponding algorithms for constructing a minimal\n",
      "supporting set of solutions can be used in solving all the considered  types\n",
      "systems and systems of mixed types.\n",
      "\n",
      "systems\n",
      "\n",
      "compatibility\n",
      "\n",
      "types\n",
      "\n",
      "linear\n",
      "\n",
      "system\n",
      "\n",
      "diophantine\n",
      "\n",
      "constraints\n",
      "\n",
      "equations\n",
      "\n",
      "nonstrict\n",
      "\n",
      "criteria\n",
      "\n",
      "natural\n",
      "\n",
      "numbers\n",
      "\n",
      "upper\n",
      "\n",
      "bounds\n",
      "\n",
      "strict\n",
      "\n",
      "inequations\n",
      "\n",
      "components\n",
      "\n",
      "solutions\n",
      "\n",
      "algorithms\n",
      "\n",
      "construction\n",
      "\n",
      "sets\n",
      "\n",
      "minimal\n",
      "\n",
      "Keywords assigned by TextRank:\n",
      "linear constraints; linear diophantine equations; natural numbers; nonstrict\n",
      "inequations; strict inequations; upper bounds\n",
      "\n",
      "Keywords assigned by human annotators:\n",
      "linear constraints; linear diophantine equations; minimal generating sets;  non−\n",
      "strict inequations; set of natural numbers; strict inequations; upper bounds\n",
      "\n",
      "Figure 2: Sample graph build for keyphrase extrac-\n",
      "tion from an Inspec abstract\n",
      "\n",
      "the text is tokenized, and annotated with part of\n",
      "speech tags – a preprocessing step required to enable\n",
      "the application of syntactic ﬁlters. To avoid exces-\n",
      "sive growth of the graph size by adding all possible\n",
      "combinations of sequences consisting of more than\n",
      "one lexical unit (ngrams), we consider only single\n",
      "words as candidates for addition to the graph, with\n",
      "multi-word keywords being eventually reconstructed\n",
      "in the post-processing phase.\n",
      "\n",
      "Next, all lexical units that pass the syntactic ﬁlter\n",
      "are added to the graph, and an edge is added between\n",
      "those lexical units that co-occur within a window of\n",
      "(cid:0) words. After the graph is constructed (undirected\n",
      "unweighted graph), the score associated with each\n",
      "vertex is set to an initial value of 1, and the ranking\n",
      "algorithm described in section 2 is run on the graph\n",
      "for several iterations until it converges – usually for\n",
      "20-30 iterations, at a threshold of 0.0001.\n",
      "\n",
      "Once a ﬁnal score is obtained for each vertex in the\n",
      "graph, vertices are sorted in reversed order of their\n",
      "score, and the top (cid:0)\n",
      "vertices in the ranking are re-\n",
      "tained for post-processing. While (cid:0) may be set to\n",
      "any ﬁxed value, usually ranging from 5 to 20 key-\n",
      "words (e.g. (Turney, 1999) limits the number of key-\n",
      "words extracted with his GenEx system to ﬁve), we\n",
      "are using a more ﬂexible approach, which decides\n",
      "\n",
      "the number of keywords based on the size of the text.\n",
      "For the data used in our experiments, which consists\n",
      "of relatively short abstracts, (cid:0)\n",
      "is set to a third of the\n",
      "number of vertices in the graph.\n",
      "\n",
      "During post-processing, all lexical units selected\n",
      "as potential keywords by the TextRank algorithm are\n",
      "marked in the text, and sequences of adjacent key-\n",
      "words are collapsed into a multi-word keyword. For\n",
      "instance, in the text Matlab code for plotting ambi-\n",
      "guity functions, if both Matlab and code are selected\n",
      "as potential keywords by TextRank, since they are\n",
      "adjacent, they are collapsed into one single keyword\n",
      "Matlab code.\n",
      "\n",
      "Figure 2 shows a sample graph built for an abstract\n",
      "from our test collection. While the size of the ab-\n",
      "stracts ranges from 50 to 350 words, with an average\n",
      "size of 120 words, we have deliberately selected a\n",
      "very small abstract for the purpose of illustration. For\n",
      "this example, the lexical units found to have higher\n",
      "“importance” by the TextRank algorithm are (with\n",
      "the TextRank score indicated in parenthesis): num-\n",
      "bers (1.46), inequations (1.45), linear (1.29), dio-\n",
      "phantine (1.28), upper (0.99), bounds (0.99), strict\n",
      "(0.77). Notice that this ranking is different than the\n",
      "one rendered by simple word frequencies. For the\n",
      "same text, a frequency approach provides the fol-\n",
      "lowing top-ranked lexical units: systems (4), types\n",
      "(3), solutions (3), minimal (3), linear (2), inequations\n",
      "(2), algorithms (2). All other lexical units have a fre-\n",
      "quency of 1, and therefore cannot be ranked, but only\n",
      "listed.\n",
      "\n",
      "3.2 Evaluation\n",
      "The data set used in the experiments is a collection\n",
      "of 500 abstracts from the Inspec database, and the\n",
      "corresponding manually assigned keywords. This is\n",
      "the same test data set as used in the keyword ex-\n",
      "traction experiments reported in (Hulth, 2003). The\n",
      "Inspec abstracts are from journal papers from Com-\n",
      "puter Science and Information Technology. Each\n",
      "abstract comes with two sets of keywords assigned\n",
      "by professional indexers: controlled keywords, re-\n",
      "stricted to a given thesaurus, and uncontrolled key-\n",
      "words, freely assigned by the indexers. We follow\n",
      "the evaluation approach from (Hulth, 2003), and use\n",
      "the uncontrolled set of keywords.\n",
      "\n",
      "In her experiments, Hulth is using a total of 2000\n",
      "abstracts, divided into 1000 for training, 500 for de-\n",
      "velopment, and 500 for test2. Since our approach\n",
      "is completely unsupervised, no training/development\n",
      "data is required, and we are only using the test docu-\n",
      "\n",
      "2Many thanks to Anette Hulth for allowing us to run our al-\n",
      "gorithm on the data set used in her keyword extraction exper-\n",
      "iments, and for making available the training/test/development\n",
      "data split.\n",
      "\n",
      "\f",
      "Method\n",
      "TextRank\n",
      "Undirected, Co-occ.window=2\n",
      "Undirected, Co-occ.window=3\n",
      "Undirected, Co-occ.window=5\n",
      "Undirected, Co-occ.window=10\n",
      "Directed, forward, Co-occ.window=2\n",
      "Directed, backward, Co-occ.window=2\n",
      "Hulth (2003)\n",
      "Ngram with tag\n",
      "NP-chunks with tag\n",
      "Pattern with tag\n",
      "\n",
      "Assigned\n",
      "Total Mean\n",
      "\n",
      "Correct\n",
      "Total Mean\n",
      "\n",
      "Precision Recall\n",
      "\n",
      "F-measure\n",
      "\n",
      "6,784\n",
      "6,715\n",
      "6,558\n",
      "6,570\n",
      "6,662\n",
      "6,636\n",
      "\n",
      "7,815\n",
      "4,788\n",
      "7,012\n",
      "\n",
      "13.7\n",
      "13.4\n",
      "13.1\n",
      "13.1\n",
      "13.3\n",
      "13.3\n",
      "\n",
      "15.6\n",
      "9.6\n",
      "14.0\n",
      "\n",
      "2,116\n",
      "1,897\n",
      "1,851\n",
      "1,846\n",
      "2,081\n",
      "2,082\n",
      "\n",
      "1,973\n",
      "1,421\n",
      "1,523\n",
      "\n",
      "4.2\n",
      "3.8\n",
      "3.7\n",
      "3.7\n",
      "4.1\n",
      "4.1\n",
      "\n",
      "3.9\n",
      "2.8\n",
      "3.1\n",
      "\n",
      "31.2\n",
      "28.2\n",
      "28.2\n",
      "28.1\n",
      "31.2\n",
      "31.2\n",
      "\n",
      "25.2\n",
      "29.7\n",
      "21.7\n",
      "\n",
      "43.1\n",
      "38.6\n",
      "37.7\n",
      "37.6\n",
      "42.3\n",
      "42.3\n",
      "\n",
      "51.7\n",
      "37.2\n",
      "39.9\n",
      "\n",
      "36.2\n",
      "32.6\n",
      "32.2\n",
      "32.2\n",
      "35.9\n",
      "35.9\n",
      "\n",
      "33.9\n",
      "33.0\n",
      "28.1\n",
      "\n",
      "Table 1: Results for automatic keyword extraction using TextRank or supervised learning (Hulth, 2003)\n",
      "\n",
      "ments for evaluation purposes.\n",
      "\n",
      "The results are evaluated using precision, recall,\n",
      "and F-measure. Notice that the maximum recall that\n",
      "can be achieved on this collection is less than 100%,\n",
      "since indexers were not limited to keyword extrac-\n",
      "tion – as our system is – but they were also allowed\n",
      "to perform keyword generation, which eventually re-\n",
      "sults in keywords that do not explicitly appear in the\n",
      "text.\n",
      "\n",
      "For comparison purposes, we are using the results\n",
      "of the state-of-the-art keyword extraction system re-\n",
      "ported in (Hulth, 2003). Shortly, her system consists\n",
      "of a supervised learning scheme that attempts to learn\n",
      "how to best extract keywords from a document, by\n",
      "looking at a set of four features that are determined\n",
      "for each “candidate” keyword: (1) within-document\n",
      "frequency, (2) collection frequency, (3) relative po-\n",
      "sition of the ﬁrst occurrence, (4) sequence of part of\n",
      "speech tags. These features are extracted from both\n",
      "training and test data for all “candidate” keywords,\n",
      "where a candidate keyword can be: Ngrams (uni-\n",
      "grams, bigrams, or trigrams extracted from the ab-\n",
      "stracts), NP-chunks (noun phrases), patterns (a set of\n",
      "part of speech patterns detected from the keywords\n",
      "attached to the training abstracts). The learning sys-\n",
      "tem is a rule induction system with bagging.\n",
      "\n",
      "Our system consists of the TextRank approach de-\n",
      "scribed in Section 3.1, with a co-occurrence window-\n",
      "size set to two, three, ﬁve, or ten words. Table 1 lists\n",
      "the results obtained with TextRank, and the best re-\n",
      "sults reported in (Hulth, 2003). For each method,\n",
      "the table lists the total number of keywords assigned,\n",
      "the mean number of keywords per abstract, the total\n",
      "number of correct keywords, as evaluated against the\n",
      "set of keywords assigned by professional indexers,\n",
      "and the mean number of correct keywords. The table\n",
      "also lists precision, recall, and F-measure.\n",
      "\n",
      "Discussion. TextRank achieves the highest preci-\n",
      "sion and F-measure across all systems, although the\n",
      "recall is not as high as in supervised methods – pos-\n",
      "\n",
      "sibly due the limitation imposed by our approach on\n",
      "the number of keywords selected, which is not made\n",
      "in the supervised system3. A larger window does not\n",
      "seem to help – on the contrary, the larger the win-\n",
      "dow, the lower the precision, probably explained by\n",
      "the fact that a relation between words that are further\n",
      "apart is not strong enough to deﬁne a connection in\n",
      "the text graph.\n",
      "\n",
      "Experiments were performed with various syntac-\n",
      "tic ﬁlters, including: all open class words, nouns and\n",
      "adjectives, and nouns only, and the best performance\n",
      "was achieved with the ﬁlter that selects nouns and ad-\n",
      "jectives only. We have also experimented with a set-\n",
      "ting where no part of speech information was added\n",
      "to the text, and all words - except a predeﬁned list\n",
      "of stopwords - were added to the graph. The re-\n",
      "sults with this setting were signiﬁcantly lower than\n",
      "the systems that consider part of speech information,\n",
      "which corroborates with previous observations that\n",
      "linguistic information helps the process of keyword\n",
      "extraction (Hulth, 2003).\n",
      "\n",
      "Experiments were also performed with directed\n",
      "graphs, where a direction was set following the natu-\n",
      "ral ﬂow of the text, e.g. one candidate keyword “rec-\n",
      "ommends” (and therefore has a directed arc to) the\n",
      "candidate keyword that follows in the text, keeping\n",
      "the restraint imposed by the co-occurrence relation.\n",
      "We have also tried the reversed direction, where a\n",
      "lexical unit points to a previous token in the text.\n",
      "Table 1 includes the results obtained with directed\n",
      "graphs for a co-occurrence window of 2. Regard-\n",
      "less of the direction chosen for the arcs, results ob-\n",
      "tained with directed graphs are worse than results ob-\n",
      "tained with undirected graphs, which suggests that\n",
      "despite a natural ﬂow in running text, there is no nat-\n",
      "ural “direction” that can be established between co-\n",
      "\n",
      "3The fact that the supervised system does not have the ca-\n",
      "pability to set a cutoff threshold on the number of keywords,\n",
      "but it only makes a binary decision on each candidate word, has\n",
      "the downside of not allowing for a precision-recall curve, which\n",
      "prohibits a comparison of such curves for the two methods.\n",
      "\n",
      "\f",
      "occurring words.\n",
      "\n",
      "Overall, our TextRank system leads to an F-\n",
      "measure higher than any of the previously proposed\n",
      "systems. Notice that TextRank is completely unsu-\n",
      "pervised, and unlike other supervised systems, it re-\n",
      "lies exclusively on information drawn from the text\n",
      "itself, which makes it easily portable to other text col-\n",
      "lections, domains, and languages.\n",
      "\n",
      "4 Sentence Extraction\n",
      "\n",
      "The other TextRank application that we investigate\n",
      "consists of sentence extraction for automatic sum-\n",
      "marization. In a way, the problem of sentence extrac-\n",
      "tion can be regarded as similar to keyword extraction,\n",
      "since both applications aim at identifying sequences\n",
      "that are more “representative” for the given text. In\n",
      "keyword extraction, the candidate text units consist\n",
      "of words or phrases, whereas in sentence extraction,\n",
      "we deal with entire sentences. TextRank turns out to\n",
      "be well suited for this type of applications, since it\n",
      "allows for a ranking over text units that is recursively\n",
      "computed based on information drawn from the en-\n",
      "tire text.\n",
      "\n",
      "4.1 TextRank for Sentence Extraction\n",
      "To apply TextRank, we ﬁrst need to build a graph as-\n",
      "sociated with the text, where the graph vertices are\n",
      "representative for the units to be ranked. For the task\n",
      "of sentence extraction, the goal is to rank entire sen-\n",
      "tences, and therefore a vertex is added to the graph\n",
      "for each sentence in the text.\n",
      "\n",
      "The co-occurrence relation used for keyword ex-\n",
      "traction cannot be applied here, since the text units in\n",
      "consideration are signiﬁcantly larger than one or few\n",
      "words, and “co-occurrence” is not a meaningful rela-\n",
      "tion for such large contexts. Instead, we are deﬁning\n",
      "a different relation, which determines a connection\n",
      "between two sentences if there is a “similarity” re-\n",
      "lation between them, where “similarity” is measured\n",
      "as a function of their content overlap. Such a rela-\n",
      "tion between two sentences can be seen as a process\n",
      "of “recommendation”: a sentence that addresses cer-\n",
      "tain concepts in a text, gives the reader a “recom-\n",
      "mendation” to refer to other sentences in the text that\n",
      "address the same concepts, and therefore a link can\n",
      "be drawn between any two such sentences that share\n",
      "common content.\n",
      "\n",
      "The overlap of two sentences can be determined\n",
      "simply as the number of common tokens between\n",
      "the lexical representations of the two sentences, or\n",
      "it can be run through syntactic ﬁlters, which only\n",
      "count words of a certain syntactic category, e.g. all\n",
      "open class words, nouns and verbs, etc. Moreover,\n",
      "to avoid promoting long sentences, we are using a\n",
      "normalization factor, and divide the content overlap\n",
      "\n",
      "3: BC−HurricaineGilbert, 09−11 339\n",
      "4: BC−Hurricaine Gilbert, 0348\n",
      "5: Hurricaine Gilbert heads toward Dominican Coast\n",
      "6: By Ruddy Gonzalez\n",
      "7: Associated Press Writer\n",
      "8: Santo Domingo, Dominican Republic (AP)\n",
      "9: Hurricaine Gilbert Swept towrd the Dominican Republic Sunday, and the Civil Defense\n",
      "    alerted its heavily populated south coast to prepare for high winds, heavy rains, and high seas.\n",
      "10: The storm was approaching from the southeast with sustained winds of 75 mph gusting\n",
      "      to 92 mph.\n",
      "11: \"There is no need for alarm,\" Civil Defense Director Eugenio Cabral said in a television \n",
      "      alert shortly after midnight Saturday.\n",
      "12: Cabral said residents of the province of Barahona should closely follow Gilbert’s movement.\n",
      "13: An estimated 100,000 people live in the province, including 70,000 in the city of Barahona,\n",
      "      about 125 miles west of Santo Domingo.\n",
      "14. Tropical storm Gilbert formed in the eastern Carribean and strenghtened into a hurricaine\n",
      "      Saturday night.\n",
      "15: The National Hurricaine Center in Miami reported its position at 2 a.m. Sunday at latitude\n",
      "      16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles\n",
      "      southeast of Santo Domingo.\n",
      "16: The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westard\n",
      "      at 15 mph with a \"broad area of cloudiness and heavy weather\" rotating around the center \n",
      "      of the storm.\n",
      "17. The weather service issued a flash flood watch for Puerto Rico and the Virgin Islands until\n",
      "       at least 6 p.m. Sunday.\n",
      "18: Strong winds associated with the Gilbert brought coastal flooding, strong southeast winds,\n",
      "      and up to 12 feet to Puerto Rico’s south coast.\n",
      "19: There were no reports on casualties.\n",
      "20: San Juan, on the north coast, had heavy rains and gusts Saturday, but they subsided during \n",
      "      the night.\n",
      "21: On Saturday, Hurricane Florence was downgraded to a tropical storm, and its remnants \n",
      "      pushed inland from the U.S. Gulf Coast. \n",
      "22: Residents returned home, happy to find little damage from 90 mph winds and sheets of rain.\n",
      "23: Florence, the sixth named storm of the 1988 Atlantic storm season, was the second hurricane.\n",
      "24: The first, Debby, reached minimal hurricane strength briefly before hitting the Mexican coast\n",
      "      last month.\n",
      "\n",
      "[0.50]\n",
      "\n",
      "24\n",
      "\n",
      "4\n",
      "\n",
      "[0.71]\n",
      "\n",
      "[0.80]\n",
      "\n",
      "23\n",
      "\n",
      "0.15\n",
      "\n",
      "5\n",
      "\n",
      "[1.20]\n",
      "\n",
      "6\n",
      "\n",
      "[0.15]\n",
      "\n",
      "[0.70]\n",
      "\n",
      "22\n",
      "\n",
      "[1.02]\n",
      "\n",
      "21\n",
      "\n",
      "0.15\n",
      "\n",
      "20\n",
      "\n",
      "[0.84]\n",
      "\n",
      "19\n",
      "\n",
      "[0.15]\n",
      "\n",
      "[1.58]\n",
      "\n",
      "18\n",
      "\n",
      "[0.70]\n",
      "\n",
      "17\n",
      "\n",
      "0.19\n",
      "\n",
      "7\n",
      "\n",
      "[0.15]\n",
      "\n",
      "0.55\n",
      "\n",
      "0.30\n",
      "\n",
      "0.59\n",
      "\n",
      "0.27\n",
      "\n",
      "0.15\n",
      "\n",
      "0.16\n",
      "\n",
      "0.15\n",
      "\n",
      "0.14\n",
      "\n",
      "8\n",
      "\n",
      "[0.70]\n",
      "\n",
      "0.35\n",
      "\n",
      "9\n",
      "\n",
      "[1.83]\n",
      "\n",
      "0.15\n",
      "\n",
      "0.29\n",
      "\n",
      "10\n",
      "\n",
      "[0.99]\n",
      "\n",
      "11\n",
      "\n",
      "[0.56]\n",
      "\n",
      "16\n",
      "[1.65]\n",
      "\n",
      "12\n",
      "\n",
      "[0.93]\n",
      "\n",
      "15\n",
      "\n",
      "[1.36]\n",
      "\n",
      "14\n",
      "\n",
      "[1.09]\n",
      "\n",
      "13\n",
      "\n",
      "[0.76]\n",
      "\n",
      "TextRank extractive summary\n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil  De−\n",
      "fense alerted its heavily populated south coast to prepare for high winds, heavy rains\n",
      "and high seas. The National Hurricane Center in Miami reported its position at 2 a.m.\n",
      "Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, \n",
      "Puerto Rico, and 200 miles southeast of Santo Domingo. The National Weather Service \n",
      "in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a \"broad\n",
      "area of cloudiness and heavy weather\" rotating around the center of the storm. Strong\n",
      "winds associated with Gilbert brought coastal flooding, strong southeast winds and up\n",
      "to 12 feet to Puerto Rico’s south coast.\n",
      "Manual abstract I\n",
      "Hurricane Gilbert is moving toward the Dominican Republic, where the residents of\n",
      "the south coast, especially the Barahona Province, have been alerted to prepare for\n",
      "heavy rains, and high wind and seas. Tropical storm Gilbert formed in the eastern\n",
      "Carribean and became a hurricane on Saturday night. By 2 a.m. Sunday it was about\n",
      "200 miles southeast of Santo Domingo and moving westward at 15 mph with winds\n",
      "of 75 mph. Flooding is expected in Puerto Rico and in the Virgin Islands. The second\n",
      "hurricane of the season, Florence, is now over the southern United States and down−\n",
      "graded to a tropical storm.\n",
      "Manual abstract II\n",
      "Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday\n",
      "night. The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to\n",
      "be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo. It\n",
      "is moving westward at 15 mph with a broad area of cloudiness and heavy weather with\n",
      "sustained winds of 75 mph gusting to 92 mph. The Dominican Republic’s Civil Defense\n",
      "alerted that country’s heavily populated south coast and the National Weather Service \n",
      "in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until\n",
      "at least 6 p.m. Sunday.\n",
      "\n",
      "Figure 3: Sample graph build for sentence extraction\n",
      "from a newspaper article. Manually assigned sum-\n",
      "maries and TextRank extractive summary are also\n",
      "shown.\n",
      "\n",
      "\f",
      "of two sentences with the length of each sentence.\n",
      "(cid:0) , with a\n",
      "Formally, given two sentences (cid:0)\n",
      "(cid:18) and (cid:0)\n",
      "(cid:18) words\n",
      "sentence being represented by the set of (cid:0)\n",
      "that appear in the sentence:\n",
      "$ ,\n",
      "the similarity of (cid:0)\n",
      "\n",
      "is deﬁned as:\n",
      "\n",
      "(cid:18) and (cid:0)\n",
      "\n",
      "(cid:4)(cid:3)(cid:5)(cid:3)(cid:5)(cid:3)\n",
      "\n",
      "(cid:8)(cid:0)\n",
      "\n",
      "(cid:21)(cid:2)\n",
      "\n",
      "(cid:21)(cid:2)\n",
      "\n",
      "(cid:0)(cid:8)(cid:7)(cid:10)(cid:9)(cid:11)(cid:7)(cid:10)(cid:12)(cid:14)(cid:13)(cid:16)(cid:15)(cid:17)(cid:7)\n",
      "\n",
      "(cid:19)(cid:18)\n",
      "\n",
      "(cid:20)(cid:0)\n",
      "\n",
      "(cid:21)(cid:0)\n",
      "\n",
      "(cid:8)(cid:28)\n",
      "\n",
      "K(cid:5)\n",
      "\n",
      "(cid:13)2(cid:3)\n",
      "\n",
      "!#\"(cid:19)$(cid:4)%\n",
      "\n",
      "!#\"(cid:19)$(cid:4)%\n",
      "\n",
      "&(cid:31)’\n",
      "\n",
      "(cid:23)(cid:25)(cid:24)\n",
      "\n",
      "(cid:31)(cid:30)\n",
      "\n",
      "(cid:27)(cid:26)(cid:29)(cid:28)\n",
      "\n",
      "(cid:27)(cid:26)(cid:29)(cid:28)\n",
      "\n",
      "(cid:27) \n",
      "\n",
      "Other sentence similarity measures, such as string\n",
      "kernels, cosine similarity, longest common subse-\n",
      "quence, etc. are also possible, and we are currently\n",
      "evaluating their impact on the summarization perfor-\n",
      "mance.\n",
      "\n",
      "The resulting graph is highly connected, with a\n",
      "weight associated with each edge,\n",
      "indicating the\n",
      "strength of the connections established between var-\n",
      "ious sentence pairs in the text. The text is therefore\n",
      "represented as a weighted graph, and consequently\n",
      "we are using the weighted graph-based ranking for-\n",
      "mula introduced in Section 2.2.\n",
      "\n",
      "After the ranking algorithm is run on the graph,\n",
      "sentences are sorted in reversed order of their score,\n",
      "and the top ranked sentences are selected for inclu-\n",
      "sion in the summary.\n",
      "\n",
      "Figure 3 shows a text sample, and the associated\n",
      "weighted graph constructed for this text. The ﬁg-\n",
      "ure also shows sample weights attached to the edges\n",
      "connected to vertex 94, and the ﬁnal TextRank score\n",
      "computed for each sentence. The sentences with the\n",
      "highest rank are selected for inclusion in the abstract.\n",
      "For this sample article, the sentences with id-s 9, 15,\n",
      "16, 18 are extracted, resulting in a summary of about\n",
      "100 words, which according to automatic evaluation\n",
      "measures, is ranked the second among summaries\n",
      "produced by 15 other systems (see Section 4.2 for\n",
      "evaluation methodology).\n",
      "\n",
      "4.2 Evaluation\n",
      "We evaluate the TextRank sentence extraction algo-\n",
      "rithm on a single-document summarization task, us-\n",
      "ing 567 news articles provided during the Document\n",
      "Understanding Evaluations 2002 (DUC, 2002). For\n",
      "each article, TextRank generates an 100-words sum-\n",
      "mary — the task undertaken by other systems partic-\n",
      "ipating in this single document summarization task.\n",
      "For evaluation, we are using the ROUGE evalu-\n",
      "ation toolkit, which is a method based on Ngram\n",
      "statistics, found to be highly correlated with hu-\n",
      "man evaluations (Lin and Hovy, 2003). Two manu-\n",
      "ally produced reference summaries are provided, and\n",
      "used in the evaluation process5.\n",
      "\n",
      "Fifteen different systems participated in this task,\n",
      "and we compare the performance of TextRank with\n",
      "the top ﬁve performing systems, as well as with the\n",
      "baseline proposed by the DUC evaluators – consist-\n",
      "ing of a 100-word summary constructed by taking\n",
      "the ﬁrst sentences in each article. Table 2 shows the\n",
      "results obtained on this data set of 567 news articles,\n",
      "including the results for TextRank (shown in bold),\n",
      "the baseline, and the results of the top ﬁve perform-\n",
      "ing systems in the DUC 2002 single document sum-\n",
      "marization task (DUC, 2002).\n",
      "\n",
      "ROUGE score – Ngram(1,1)\n",
      "\n",
      "System\n",
      "\n",
      "S27\n",
      "S31\n",
      "TextRank\n",
      "S28\n",
      "S21\n",
      "Baseline\n",
      "S29\n",
      "\n",
      "basic\n",
      "(a)\n",
      "0.4814\n",
      "0.4715\n",
      "0.4708\n",
      "0.4703\n",
      "0.4683\n",
      "0.4599\n",
      "0.4502\n",
      "\n",
      "stemmed\n",
      "(b)\n",
      "0.5011\n",
      "0.4914\n",
      "0.4904\n",
      "0.4890\n",
      "0.4869\n",
      "0.4779\n",
      "0.4681\n",
      "\n",
      "stemmed\n",
      "no-stopwords\n",
      "(c)\n",
      "0.4405\n",
      "0.4160\n",
      "0.4229\n",
      "0.4346\n",
      "0.4222\n",
      "0.4162\n",
      "0.4019\n",
      "\n",
      "Table 2: Results for single document summarization:\n",
      "TextRank, top 5 (out of 15) DUC 2002 systems, and\n",
      "baseline. Evaluation takes into account (a) all words;\n",
      "(b) stemmed words; (c) stemmed words, and no stop-\n",
      "words.\n",
      "\n",
      "Discussion. TextRank succeeds in identifying the\n",
      "most important sentences in a text based on infor-\n",
      "mation exclusively drawn from the text itself. Un-\n",
      "like other supervised systems, which attempt to learn\n",
      "what makes a good summary by training on collec-\n",
      "tions of summaries built for other articles, TextRank\n",
      "is fully unsupervised, and relies only on the given\n",
      "text to derive an extractive summary, which repre-\n",
      "sents a summarization model closer to what humans\n",
      "are doing when producing an abstract for a given\n",
      "document.\n",
      "\n",
      "Notice that TextRank goes beyond the sentence\n",
      "“connectivity” in a text. For instance, sentence 15 in\n",
      "the example provided in Figure 3 would not be iden-\n",
      "tiﬁed as “important” based on the number of connec-\n",
      "tions it has with other vertices in the graph, but it is\n",
      "identiﬁed as “important” by TextRank (and by hu-\n",
      "mans – see the reference summaries displayed in the\n",
      "same ﬁgure).\n",
      "\n",
      "Another important aspect of TextRank is that it\n",
      "gives a ranking over all sentences in a text – which\n",
      "means that it can be easily adapted to extracting\n",
      "very short summaries (headlines consisting of one\n",
      "\n",
      "4Weights are listed to the right or above the edge they cor-\n",
      "respond to. Similar weights are computed for each edge in the\n",
      "graph, but are not displayed due to space restrictions.\n",
      "\n",
      "5ROUGE is available at http://www.isi.edu/˜cyl/ROUGE/.\n",
      "\n",
      "The evaluation is done using the Ngram(1,1) setting of ROUGE,\n",
      "which was found to have the highest correlation with human\n",
      "judgments, at a conﬁdence level of 95%. Only the ﬁrst 100\n",
      "words in each summary are considered.\n",
      "\n",
      "(cid:0)\n",
      "(cid:18)\n",
      "(cid:3)\n",
      "(cid:2)\n",
      "(cid:18)\n",
      "(cid:1)\n",
      "(cid:9)\n",
      "(cid:18)\n",
      "(cid:2)\n",
      "(cid:9)\n",
      "(cid:9)\n",
      "(cid:18)\n",
      "(cid:6)\n",
      "(cid:18)\n",
      "(cid:9)\n",
      "(cid:0)\n",
      "(cid:22)\n",
      "(cid:4)\n",
      "(cid:22)\n",
      "(cid:24)\n",
      "(cid:4)\n",
      "$\n",
      "(cid:24)\n",
      "(cid:4)\n",
      "4\n",
      "(cid:22)\n",
      "(cid:22)\n",
      "(cid:28)\n",
      "$\n",
      "(cid:22)\n",
      "(cid:22)\n",
      "(cid:28)\n",
      "4\n",
      "(cid:22)\n",
      "&\n",
      "\f",
      "sentence), or longer more explicative summaries,\n",
      "consisting of more than 100 words. We are also\n",
      "investigating combinations of keyphrase and sen-\n",
      "tence extraction techniques as a method for building\n",
      "short/long summaries.\n",
      "\n",
      "Finally, another advantage of TextRank over previ-\n",
      "ously proposed methods for building extractive sum-\n",
      "maries is the fact that it does not require training cor-\n",
      "pora, which makes it easily adaptable to other lan-\n",
      "guages or domains.\n",
      "\n",
      "5 Why TextRank Works\n",
      "Intuitively, TextRank works well because it does not\n",
      "only rely on the local context of a text unit (vertex),\n",
      "but rather it takes into account information recur-\n",
      "sively drawn from the entire text (graph).\n",
      "\n",
      "Through the graphs it builds on texts, TextRank\n",
      "identiﬁes connections between various entities in a\n",
      "text, and implements the concept of recommenda-\n",
      "tion. A text unit recommends other related text\n",
      "units, and the strength of the recommendation is re-\n",
      "cursively computed based on the importance of the\n",
      "units making the recommendation. For instance, in\n",
      "the keyphrase extraction application, co-occurring\n",
      "words recommend each other as important, and it is\n",
      "the common context that enables the identiﬁcation of\n",
      "connections between words in text. In the process of\n",
      "identifying important sentences in a text, a sentence\n",
      "recommends another sentence that addresses similar\n",
      "concepts as being useful for the overall understand-\n",
      "ing of the text. The sentences that are highly recom-\n",
      "mended by other sentences in the text are likely to\n",
      "be more informative for the given text, and will be\n",
      "therefore given a higher score.\n",
      "\n",
      "An analogy can be also drawn with PageRank’s\n",
      "“random surfer model”, where a user surfs the Web\n",
      "by following links from any given Web page. In the\n",
      "context of text modeling, TextRank implements what\n",
      "we refer to as “text surﬁng”, which relates to the con-\n",
      "cept of text cohesion (Halliday and Hasan, 1976):\n",
      "in a text, we are likely to\n",
      "from a certain concept (cid:0)\n",
      "“follow” links to connected concepts – that is, con-\n",
      "cepts that have a relation with the current concept (cid:0)\n",
      "(be that a lexical or semantic relation). This also re-\n",
      "lates to the “knitting” phenomenon (Hobbs, 1974):\n",
      "facts associated with words are shared in different\n",
      "parts of the discourse, and such relationships serve\n",
      "to “knit the discourse together”.\n",
      "\n",
      "Through its iterative mechanism, TextRank goes\n",
      "beyond simple graph connectivity, and it is able to\n",
      "score text units based also on the “importance” of\n",
      "other text units they link to. The text units selected by\n",
      "TextRank for a given application are the ones most\n",
      "recommended by related text units in the text, with\n",
      "preference given to the recommendations made by\n",
      "\n",
      "the ones that are in turn\n",
      "most inﬂuential ones, i.e.\n",
      "highly recommended by other related units. The un-\n",
      "derlying hypothesis is that in a cohesive text frag-\n",
      "ment, related text units tend to form a “Web” of con-\n",
      "nections that approximates the model humans build\n",
      "about a given context in the process of discourse un-\n",
      "derstanding.\n",
      "\n",
      "6 Conclusions\n",
      "In this paper, we introduced TextRank – a graph-\n",
      "based ranking model for text processing, and show\n",
      "how it can be successfully used for natural language\n",
      "In particular, we proposed and eval-\n",
      "applications.\n",
      "uated two innovative unsupervised approaches for\n",
      "keyword and sentence extraction, and showed that\n",
      "the accuracy achieved by TextRank in these applica-\n",
      "tions is competitive with that of previously proposed\n",
      "state-of-the-art algorithms. An important aspect of\n",
      "TextRank is that it does not require deep linguistic\n",
      "knowledge, nor domain or language speciﬁc anno-\n",
      "tated corpora, which makes it highly portable to other\n",
      "domains, genres, or languages.\n",
      "\n",
      "References\n",
      "S. Brin and L. Page. 1998. The anatomy of a large-scale hyper-\n",
      "textual Web search engine. Computer Networks and ISDN\n",
      "Systems, 30(1–7).\n",
      "\n",
      "DUC.\n",
      "\n",
      "2002. Document understanding conference 2002.\n",
      "\n",
      "http://www-nlpir.nist.gov/projects/duc/.\n",
      "\n",
      "E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G.\n",
      "Nevill-Manning. 1999. Domain-speciﬁc keyphrase extrac-\n",
      "tion. In Proceedings of the 16th International Joint Confer-\n",
      "ence on Artiﬁcial Intelligence.\n",
      "\n",
      "M. Halliday and R. Hasan. 1976. Cohesion in English. Long-\n",
      "\n",
      "man.\n",
      "\n",
      "P.J. Herings, G. van der Laan, and D. Talman. 2001. Measuring\n",
      "the power of nodes in digraphs. Technical report, Tinbergen\n",
      "Institute.\n",
      "\n",
      "J. Hobbs. 1974. A model for natural language semantics. Part I:\n",
      "\n",
      "The model. Technical report, Yale University.\n",
      "\n",
      "A. Hulth. 2003. Improved automatic keyword extraction given\n",
      "more linguistic knowledge. In Proceedings of the 2003 Con-\n",
      "ference on Empirical Methods in Natural Language Process-\n",
      "ing, Japan, August.\n",
      "\n",
      "J.M. Kleinberg. 1999. Authoritative sources in a hyperlinked\n",
      "\n",
      "environment. Journal of the ACM, 46(5):604–632.\n",
      "\n",
      "C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of sum-\n",
      "maries using n-gram co-occurrence statistics. In Proceedings\n",
      "of Human Language Technology Conference (HLT-NAACL\n",
      "2003), Edmonton, Canada, May.\n",
      "\n",
      "R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on semantic\n",
      "networks, with application to word sense disambiguation. In\n",
      "Proceedings of the 20st International Conference on Compu-\n",
      "tational Linguistics (COLING 2004), Geneva, Switzerland.\n",
      "R. Mihalcea. 2004. Graph-based ranking algorithms for sen-\n",
      "tence extraction, applied to text summarization.\n",
      "In Pro-\n",
      "ceedings of the 42nd Annual Meeting of the Association for\n",
      "Computational Lingusitics (ACL 2004) (companion volume),\n",
      "Barcelona, Spain.\n",
      "\n",
      "P. Turney. 1999. Learning to extract keyphrases from text.\n",
      "Technical report, National Research Council, Institute for In-\n",
      "formation Technology.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convert_pdf_to_string(file_path):\n",
    "    output_string = StringIO()\n",
    "    with open(file_path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "    return(output_string.getvalue())\n",
    "\n",
    "file_path = ''  # !\n",
    "text = convert_pdf_to_string(\"textrank algorithm paper.pdf\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b4945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Capstone]",
   "language": "python",
   "name": "conda-env-Capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
