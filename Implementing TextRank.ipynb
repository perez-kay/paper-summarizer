{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29103751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizing import get_tokenized_sentences, get_raw_sentences\n",
    "\n",
    "raw_sentences = get_raw_sentences(\"preprocessed/textrank.txt\") \n",
    "sentence_tokens = get_tokenized_sentences(raw_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdefe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395a38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in sentence_tokens:\n",
    "    if len(sentence) != 0:\n",
    "        v = sum([word_embeddings.get(word, np.zeros((100,))) for word in sentence])/(len(sentence)+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843971e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_mat = np.zeros([len(raw_sentences), len(raw_sentences)])\n",
    "for i in range(len(raw_sentences)):\n",
    "    for j in range(len(raw_sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "812b3cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc. Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.\n",
      "\n",
      "4 The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc. Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap Figure 3: Sample graph build for sentence extraction from a newspaper article.\n",
      "\n",
      "Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between co 3The fact that the supervised system does not have the capability to set a cutoff threshold on the number of keywords, but it only makes a binary decision on each candidate word, has the downside of not allowing for a precision-recall curve, which prohibits a comparison of such curves for the two methods.\n",
      "\n",
      "Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.\n",
      "\n",
      "Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.\n",
      "\n",
      "Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).\n",
      "\n",
      "The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.\n",
      "\n",
      "While T may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g. (Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.\n",
      "\n",
      "After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges — usually for 20-30 iterations, at a threshold of 0.0001.\n",
      "\n",
      "For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "short = round(len(raw_sentences) * 0.1)\n",
    "medium = round(len(raw_sentences) * 0.2)\n",
    "long = round(len(raw_sentences) * 0.3)\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(raw_sentences)), reverse=True)\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f33bc57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.\n",
      "\n",
      "Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.\n",
      "\n",
      "In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability d, and jumps to a completely new page with probability 1— d. The factor d is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.\n",
      "\n",
      "However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.\n",
      "\n",
      "Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc. Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.\n",
      "\n",
      "To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.\n",
      "\n",
      "While T may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g. (Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.\n",
      "\n",
      "Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction — as our system is — but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.\n",
      "\n",
      "Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.\n",
      "\n",
      "Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between co 3The fact that the supervised system does not have the capability to set a cutoff threshold on the number of keywords, but it only makes a binary decision on each candidate word, has the downside of not allowing for a precision-recall curve, which prohibits a comparison of such curves for the two methods.\n",
      "\n",
      "In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.\n",
      "\n",
      "TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.\n",
      "\n",
      "The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.\n",
      "\n",
      "4 The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc. Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap Figure 3: Sample graph build for sentence extraction from a newspaper article.\n",
      "\n",
      "Another important aspect of TextRank is that it gives a ranking over all sentences in a text — which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.\n",
      "\n",
      "Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).\n",
      "\n",
      "Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_sentences = sorted(((scores[i],s) for i,s in enumerate(raw_sentences)), reverse=True)\n",
    "\n",
    "\n",
    "ranked_sentences = [sentence for score, sentence in scored_sentences]\n",
    "\n",
    "size = short\n",
    "for i in range(len(raw_sentences)):\n",
    "    if raw_sentences[i] in ranked_sentences[:size]:\n",
    "        print(raw_sentences[i])\n",
    "        print()\n",
    "\n",
    "#print(ranked_sentences[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Capstone]",
   "language": "python",
   "name": "conda-env-Capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
