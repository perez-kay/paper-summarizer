{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29103751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizing import get_tokenized_sentences, get_raw_sentences\n",
    "\n",
    "raw_sentences = get_raw_sentences(\"preprocessed/variations.txt\") \n",
    "sentence_tokens = get_tokenized_sentences(raw_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdefe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395a38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in sentence_tokens:\n",
    "    if len(sentence) != 0:\n",
    "        v = sum([word_embeddings.get(word, np.zeros((100,))) for word in sentence])/(len(sentence)+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843971e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_mat = np.zeros([len(raw_sentences), len(raw_sentences)])\n",
    "for i in range(len(raw_sentences)):\n",
    "    for j in range(len(raw_sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812b3cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results obtained and the examples analyzed show that this variation is better than the original TextRank algorithm without a performance penalty.\n",
      "\n",
      "Several statistical models have been developed based on training corpora to combine different heuristics using keywords, position and length of sentences, word frequency or titles [13].\n",
      "\n",
      "Using different levels of compression, a summarized version of the document of arbitrary length can be obtained.\n",
      "\n",
      "Thus it is able to capture and express richer information in determining important concepts The selected text fragments to use in the graph construction can be phrases [6], sentences or paragraphs .\n",
      "\n",
      "The three alternatives improved significantly the results of the algorithm using the same test configuration as in the original publication.\n",
      "\n",
      "These features makes the algorithm widely used: it performs well summarizing structured text like news articles, but it has also shown good results in other usages such as summarizing meeting transcriptions [8] and assessing web content credibility In this article we present different proposals for the construction of the graph and report the results obtained with them.\n",
      "\n",
      "To check the correct behaviour of our test suite we implemented the reference method used in [17], which extracts the first sentences of each document.\n",
      "\n",
      "These algorithms use different information retrieval techniques to determine the most important sentences (vertices) and build the summary .\n",
      "\n",
      "We also contributed the BM25-TextRank algorithm to the Gensim project] This work presented three different variations to the TextRank algorithm for automatic summarization.\n",
      "\n",
      "According to these approach the most important sentences are the most connected ones in the graph and are used for building a final summary [2].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(raw_sentences)), reverse=True)\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b93e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using different levels of compression, a summarized version of the document of arbitrary length can be obtained.\n",
      "\n",
      "These features makes the algorithm widely used: it performs well summarizing structured text like news articles, but it has also shown good results in other usages such as summarizing meeting transcriptions [8] and assessing web content credibility In this article we present different proposals for the construction of the graph and report the results obtained with them.\n",
      "\n",
      "The first sections of this article describe previous work in the area and an overview of the TextRank algorithm.\n",
      "\n",
      "Several statistical models have been developed based on training corpora to combine different heuristics using keywords, position and length of sentences, word frequency or titles [13].\n",
      "\n",
      "Thus it is able to capture and express richer information in determining important concepts The selected text fragments to use in the graph construction can be phrases [6], sentences or paragraphs .\n",
      "\n",
      "According to these approach the most important sentences are the most connected ones in the graph and are used for building a final summary [2].\n",
      "\n",
      "Also, some authors have proposed combinations of the previous with supervised learning functions [19].\n",
      "\n",
      "These algorithms use different information retrieval techniques to determine the most important sentences (vertices) and build the summary .\n",
      "\n",
      "Other graph-based ranking algorithms such as HITS or Positional Function may be also applied.\n",
      "\n",
      "This produces a ranking of the elements in the graph: the most important elements are the ones that better describe the text.\n",
      "\n",
      "This approach allows TextRank to build summaries without the need of a training corpus or labeling and allows the use of the algorithm with different languages.\n",
      "\n",
      "This function is used to weight the graph edges, the higher the similarity between sentences the more important The function featured in the original algorithm can be formalized as: the edge between them will be in the graph.\n",
      "\n",
      "This section will describe the different modifications that we propose over the original TextRank algorithm.\n",
      "\n",
      "This function definition implies that if a word appears in more than half the documents of the collection, it will have a negative value.\n",
      "\n",
      "To check the correct behaviour of our test suite we implemented the reference method used in [17], which extracts the first sentences of each document.\n",
      "\n",
      "3 Source code available at: | * Source code available at: | A reference implementation of our proposals was coded as a Python moduld] and can be obtained for testing and to reproduce results.\n",
      "\n",
      "We also contributed the BM25-TextRank algorithm to the Gensim project] This work presented three different variations to the TextRank algorithm for automatic summarization.\n",
      "\n",
      "The three alternatives improved significantly the results of the algorithm using the same test configuration as in the original publication.\n",
      "\n",
      "The combination of TextRank with modern Information Retrieval ranking functions such as BM25 and BM25+ creates a robust method for automatic summarization that performs better than the standard techniques used previously.\n",
      "\n",
      "The results obtained and the examples analyzed show that this variation is better than the original TextRank algorithm without a performance penalty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_sentences = sorted(((scores[i],s) for i,s in enumerate(raw_sentences)), reverse=True)\n",
    "\n",
    "num_to_print = 10\n",
    "ranked_sentences = [sentence for score, sentence in scored_sentences]\n",
    "\n",
    "# num_printed = 0\n",
    "for i in range(len(raw_sentences)):\n",
    "    if raw_sentences[i] in ranked_sentences[:20]:\n",
    "        print(raw_sentences[i])\n",
    "        print()\n",
    "\n",
    "#print(ranked_sentences[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Capstone]",
   "language": "python",
   "name": "conda-env-Capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
